# -
这是一个“最小可运行”的本地 LLM 调用模板：前端点击按钮，通过 FastAPI 调用本机 Ollama，强制返回严格 JSON，再将结果渲染为卡片列表。项目包含健康检查、JSON 兜底解析、CORS 直连、历史记录、导出/复制等功能，开箱即用，适合用来快速验证本地模型能力或作为全栈项目的起步骨架
